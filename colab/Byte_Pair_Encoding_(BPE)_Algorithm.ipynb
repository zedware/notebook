{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zedware/notebook/blob/master/colab/Byte_Pair_Encoding_(BPE)_Algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Aloha, this is generated by Google Gemini.\"\"\"\n",
        "import re\n",
        "import collections\n",
        "\n",
        "def get_word_counts(text):\n",
        "    \"\"\"\n",
        "    Counts the frequency of each 'word' in the text.\n",
        "    We use a simple regex to split text into words and punctuation.\n",
        "    This pre-tokenization step is important.\n",
        "    \"\"\"\n",
        "    # Find all sequences of letters/numbers (words) or any single non-whitespace/non-word char (punctuation)\n",
        "    words = re.findall(r\"\\w+|[^\\s\\w]+\", text)\n",
        "    return collections.Counter(words)\n",
        "\n",
        "def initialize_splits(word_counts):\n",
        "    \"\"\"\n",
        "    Initializes the 'splits' dictionary.\n",
        "    Each word is split into a list of its individual characters.\n",
        "    Example: \"low\" -> ['l', 'o', 'w']\n",
        "    \"\"\"\n",
        "    return {word: list(word) for word in word_counts.keys()}\n",
        "\n",
        "def get_pair_frequencies(splits, word_counts):\n",
        "    \"\"\"\n",
        "    Counts the frequency of each adjacent pair of tokens in the corpus.\n",
        "    This is the core of the BPE algorithm.\n",
        "    \"\"\"\n",
        "    pair_freqs = collections.defaultdict(int)\n",
        "    for word, count in word_counts.items():\n",
        "        tokens = splits[word]\n",
        "        # Iterate through adjacent pairs\n",
        "        for i in range(len(tokens) - 1):\n",
        "            pair = (tokens[i], tokens[i+1])\n",
        "            pair_freqs[pair] += count\n",
        "    return pair_freqs\n",
        "\n",
        "def merge_pair(best_pair, splits):\n",
        "    \"\"\"\n",
        "    Merges the 'best_pair' in all words in our 'splits' dictionary.\n",
        "    Example: if best_pair = ('l', 'o'),\n",
        "    ['l', 'o', 'w'] becomes ['lo', 'w']\n",
        "    \"\"\"\n",
        "    new_token = \"\".join(best_pair)\n",
        "    new_splits = {}\n",
        "    for word, tokens in splits.items():\n",
        "        new_tokens = []\n",
        "        i = 0\n",
        "        while i < len(tokens):\n",
        "            # Check if we found the pair to merge\n",
        "            if i < len(tokens) - 1 and (tokens[i], tokens[i+1]) == best_pair:\n",
        "                new_tokens.append(new_token)\n",
        "                i += 2  # Skip both tokens\n",
        "            else:\n",
        "                new_tokens.append(tokens[i])\n",
        "                i += 1\n",
        "        new_splits[word] = new_tokens\n",
        "    return new_splits\n",
        "\n",
        "def train_bpe(text, vocab_size):\n",
        "    \"\"\"\n",
        "    Trains a BPE tokenizer from a text corpus.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text corpus.\n",
        "        vocab_size (int): The desired final vocabulary size.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (merges, vocab)\n",
        "            merges (dict): A dictionary of merge rules, e.g., {('l', 'o'): 'lo'}\n",
        "            vocab (set): The final vocabulary of tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- 1. Pre-tokenization and Initialization ---\n",
        "\n",
        "    # Get word counts\n",
        "    word_counts = get_word_counts(text)\n",
        "\n",
        "    # Initialize how each word is split (initially, into characters)\n",
        "    splits = initialize_splits(word_counts)\n",
        "\n",
        "    # Initialize the vocabulary with all unique characters\n",
        "    vocab = set()\n",
        "    for word in word_counts:\n",
        "        vocab.update(list(word))\n",
        "\n",
        "    # --- 2. Iterative Merging ---\n",
        "\n",
        "    # Calculate how many merges we need to do\n",
        "    num_merges = vocab_size - len(vocab)\n",
        "\n",
        "    # We store merge rules in an ordered dict (Python 3.7+ dicts are ordered)\n",
        "    # This preserves the merge priority, which is crucial for encoding.\n",
        "    merges = {}\n",
        "\n",
        "    for i in range(num_merges):\n",
        "        # Find the most frequent pair\n",
        "        pair_freqs = get_pair_frequencies(splits, word_counts)\n",
        "\n",
        "        # If there are no more pairs, we're done\n",
        "        if not pair_freqs:\n",
        "            print(\"No more pairs to merge. Stopping early.\")\n",
        "            break\n",
        "\n",
        "        best_pair = max(pair_freqs, key=pair_freqs.get)\n",
        "        new_token = \"\".join(best_pair)\n",
        "\n",
        "        # Print the merge\n",
        "        print(f\"Merge {i+1}/{num_merges}: {best_pair} -> {new_token} (freq: {pair_freqs[best_pair]})\")\n",
        "\n",
        "        # Apply this merge to our 'splits' data\n",
        "        splits = merge_pair(best_pair, splits)\n",
        "\n",
        "        # Save the merge rule and add the new token to the vocab\n",
        "        merges[best_pair] = new_token\n",
        "        vocab.add(new_token)\n",
        "\n",
        "    return merges, vocab\n",
        "\n",
        "def encode(text, merges):\n",
        "    \"\"\"\n",
        "    Encodes a new string using the learned merge rules.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to encode.\n",
        "        merges (dict): The learned merge rules.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    # Pre-tokenize the text into 'words'\n",
        "    words = re.findall(r\"\\w+|[^\\s\\w]+\", text)\n",
        "\n",
        "    final_tokens = []\n",
        "\n",
        "    for word in words:\n",
        "        # Start by splitting the word into characters\n",
        "        tokens = list(word)\n",
        "\n",
        "        # Keep applying merges until no more merges are possible\n",
        "        while True:\n",
        "            # Find the first merge rule that can be applied\n",
        "            best_pair = None\n",
        "            min_priority = float('inf')\n",
        "\n",
        "            # Find the highest-priority (earliest learned) merge\n",
        "            for i in range(len(tokens) - 1):\n",
        "                pair = (tokens[i], tokens[i+1])\n",
        "                if pair in merges:\n",
        "                    # Find the priority (order) of this merge\n",
        "                    priority = list(merges.keys()).index(pair)\n",
        "                    if priority < min_priority:\n",
        "                        min_priority = priority\n",
        "                        best_pair = pair\n",
        "                        best_pair_idx = i\n",
        "\n",
        "            # If no merge rules can be applied to this word, we're done\n",
        "            if best_pair is None:\n",
        "                break\n",
        "\n",
        "            # Apply the highest-priority merge\n",
        "            new_token = merges[best_pair]\n",
        "            # Rebuild the token list\n",
        "            tokens = tokens[:best_pair_idx] + [new_token] + tokens[best_pair_idx+2:]\n",
        "\n",
        "        # Add the fully tokenized word to our final list\n",
        "        final_tokens.extend(tokens)\n",
        "\n",
        "    return final_tokens\n",
        "\n",
        "# --- Main execution ---\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # 1. Define a sample corpus\n",
        "    corpus = (\n",
        "        \"low low low lower lowest\\n\"\n",
        "        \"new newer newest\\n\"\n",
        "        \"slow slower slowest\\n\"\n",
        "        \"SLOWLY FOR FUN\"\n",
        "    )\n",
        "\n",
        "    # 2. Train the tokenizer\n",
        "    # We want a small vocab size for this demo\n",
        "    # Initial chars: l, o, w, e, r, s, t, n, (and the capital letters)\n",
        "    # (approx 10 chars)\n",
        "    # Let's aim for a total vocab of 20\n",
        "    target_vocab_size = 20\n",
        "\n",
        "    print(\"--- Training BPE ---\")\n",
        "    merges, vocab = train_bpe(corpus, target_vocab_size)\n",
        "    print(\"--------------------\")\n",
        "\n",
        "    # 3. Show the results\n",
        "    print(f\"\\nFinal Vocabulary ({len(vocab)} tokens):\")\n",
        "    # Sort for readability\n",
        "    print(sorted(list(vocab)))\n",
        "\n",
        "    print(\"\\nLearned Merge Rules (in order of priority):\")\n",
        "    print(merges)\n",
        "\n",
        "    # 4. Test the encoder\n",
        "    print(\"\\n--- Encoding Examples ---\")\n",
        "\n",
        "    text1 = \"low\"\n",
        "    print(f\"'{text1}' -> {encode(text1, merges)}\")\n",
        "\n",
        "    text2 = \"aloha\"\n",
        "    print(f\"'{text2}' -> {encode(text2, merges)}\")\n",
        "\n",
        "    text3 = \"FUNNY\"\n",
        "    print(f\"'{text3}' -> {encode(text3, merges)}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Training BPE ---\n",
            "Merge 1/3: ('l', 'o') -> lo (freq: 8)\n",
            "Merge 2/3: ('lo', 'w') -> low (freq: 8)\n",
            "Merge 3/3: ('low', 'e') -> lowe (freq: 4)\n",
            "--------------------\n",
            "\n",
            "Final Vocabulary (20 tokens):\n",
            "['F', 'L', 'N', 'O', 'R', 'S', 'U', 'W', 'Y', 'e', 'l', 'lo', 'low', 'lowe', 'n', 'o', 'r', 's', 't', 'w']\n",
            "\n",
            "Learned Merge Rules (in order of priority):\n",
            "{('l', 'o'): 'lo', ('lo', 'w'): 'low', ('low', 'e'): 'lowe'}\n",
            "\n",
            "--- Encoding Examples ---\n",
            "'low' -> ['low']\n",
            "'aloha' -> ['a', 'lo', 'h', 'a']\n",
            "'FUNNY' -> ['F', 'U', 'N', 'N', 'Y']\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHG4632uUonj",
        "outputId": "0ce5026e-efcc-491b-9829-0e7a537786ee"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}